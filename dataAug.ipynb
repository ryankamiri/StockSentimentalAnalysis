{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from model import makeModel\n",
    "import helper\n",
    "import nlpaug.augmenter.word as naw\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/training_data.json\", 'r') as f:\n",
    "    datastore = json.load(f)\n",
    "\n",
    "headlines = []\n",
    "labels = []\n",
    "\n",
    "for item in datastore:\n",
    "    headlines.append(item['title'])\n",
    "    labels.append(item['good_news'])\n",
    "\n",
    "# split the data 80/20\n",
    "training_headlines, testing_headlines, training_labels, testing_labels = train_test_split(headlines, labels, train_size=0.8)\n",
    "\n",
    "#limiting character length and vocab size\n",
    "vocab_size = 10000\n",
    "max_length = 25\n",
    "embedding_dim = 10\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "\n",
    "# training tokenizer\n",
    "tokenizer.fit_on_texts(headlines)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "\n",
    "\n",
    "TOPK=20 #default=100\n",
    "ACT = 'insert' #\"substitute\"\n",
    "training_headlines_aug=[] \n",
    "\n",
    "# with open(r'data/augmented_training_data.txt', 'w', encoding=\"utf-8\") as fp:\n",
    "\n",
    "#     for i in range(len(training_headlines)):\n",
    "#         aug = naw.ContextualWordEmbsAug(\n",
    "#             model_path='bert-base-uncased', action=\"insert\")\n",
    "#         fp.write(str(aug.augment(training_headlines[i])) + \"\\n\")\n",
    "#         print(str(i) + \" out of \" + str(len(training_headlines)))\n",
    "\n",
    "#print(\"------------------------------------------------------------------------------\")\n",
    "#print(\"Augmented Text:\")\n",
    "#print(training_headlines_aug[0:10])\n",
    "\n",
    "with open('data/augmented_training_data.txt', 'r', encoding='UTF-8') as file:\n",
    "    for line in file:\n",
    "        training_headlines_aug.append(line)\n",
    "    \n",
    "file.close()\n",
    "#print(training_headlines_aug[0])\n",
    "\n",
    "# adding padding \n",
    "training_sequences = tokenizer.texts_to_sequences(training_headlines_aug)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_headlines)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "training_labels = np.array(training_labels)\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = makeModel(vocab_size, embedding_dim, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(training_padded, training_labels, batch_size=8,epochs=40, validation_data=(testing_padded, testing_labels), callbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.plot_graphs(history, \"accuracy\")\n",
    "helper.plot_graphs(history, \"loss\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
