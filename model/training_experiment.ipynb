{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import json\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postive: 2085, Negative: 3728\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/training_data_new.json\", 'r') as f:\n",
    "    datastore = json.load(f)\n",
    "\n",
    "with open(\"./data/training_data_pj.json\", 'r') as f:\n",
    "    datastore += json.load(f)\n",
    "\n",
    "headlines = []\n",
    "labels = []\n",
    "\n",
    "for item in datastore:\n",
    "    headlines.append(item['title'])\n",
    "    labels.append(item['good_news'])\n",
    "\n",
    "with open(\"./data/kaggle.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=\",\")\n",
    "    for line in reader:\n",
    "        label = line[0]\n",
    "        headline = line[1]\n",
    "        # if random.randint(0, 1) >= .5:\n",
    "        #     continue\n",
    "        if label == \"positive\":\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "        headlines.append(headline)\n",
    "\n",
    "print(f\"Postive: {labels.count(1)}, Negative: {labels.count(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download nltk resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words and contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = False\n",
    "stem = True\n",
    "v2 = True\n",
    "\n",
    "if v2:\n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "else:\n",
    "    stemmer = PorterStemmer()\n",
    "lemmmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preProcess(sentence):\n",
    "    words = []\n",
    "    for word in sentence.split():\n",
    "        #Take out all non letter characters\n",
    "        #word = re.sub(\"[^a-zA-Z]+\", \"\", word.lower())\n",
    "        word = word.lower()\n",
    "        if word not in stopwords.words('english') and word != \"\":\n",
    "            if lem:\n",
    "                newWord = lemmmatizer.lemmatize(word)\n",
    "            elif stem:\n",
    "                newWord = stemmer.stem(word)\n",
    "            else:\n",
    "                newWord = word\n",
    "            words.append(newWord.replace(\"'\", \"\"))\n",
    "\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(headlines)):\n",
    "    headlines[i] = preProcess(headlines[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 5231 Training sets and 582 Testing sets.\n"
     ]
    }
   ],
   "source": [
    "training_headlines, testing_headlines, training_labels, testing_labels = train_test_split(headlines, labels, train_size=0.90)\n",
    "print(f\"Split into {len(training_headlines)} Training sets and {len(testing_headlines)} Testing sets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune Hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "max_length = 35\n",
    "trunc_type = \"post\"\n",
    "padding_type = \"post\"\n",
    "oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Index Size: 8916\n"
     ]
    }
   ],
   "source": [
    "def tune_hyper(vocab_size, max_length, training_headlines, testing_headlines, training_labels, testing_labels):    \n",
    "    \n",
    "\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "\n",
    "    # training tokenizer\n",
    "    #tokenizer.word_index = word_index\n",
    "\n",
    "    tokenizer.fit_on_texts(training_headlines)\n",
    "\n",
    "    print(f\"Word Index Size: {len(tokenizer.word_index)}\")\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = tune_hyper(vocab_size, max_length, training_headlines, testing_headlines, training_labels, testing_labels)\n",
    "\n",
    "# adding padding \n",
    "training_sequences = tokenizer.texts_to_sequences(training_headlines)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_headlines)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "training_labels = np.array(training_labels)\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define MCC Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5  \n",
    "\n",
    "def mcc_metric(y_true, y_pred):\n",
    "  predicted = tf.cast(tf.greater(y_pred, threshold), tf.float32)\n",
    "  true_pos = tf.math.count_nonzero(predicted * y_true)\n",
    "  true_neg = tf.math.count_nonzero((predicted - 1) * (y_true - 1))\n",
    "  false_pos = tf.math.count_nonzero(predicted * (y_true - 1))\n",
    "  false_neg = tf.math.count_nonzero((predicted - 1) * y_true)\n",
    "  x = tf.cast((true_pos + false_pos) * (true_pos + false_neg) \n",
    "      * (true_neg + false_pos) * (true_neg + false_neg), tf.float32)\n",
    "  return tf.cast((true_pos * true_neg) - (false_pos * false_neg), tf.float32) / tf.sqrt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "baseline = tf.keras.Sequential([\n",
    "    # tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    # #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(300)),#, return_sequences=False, dropout=0.25, recurrent_dropout=0.25)),\n",
    "    # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True)),\n",
    "    # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True)),\n",
    "    # tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    # tf.keras.layers.Dropout(0.5),\n",
    "    # #tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    # #tf.keras.layers.Dropout(0.5),\n",
    "    # tf.keras.layers.Dense(4, activation=\"relu\"),\n",
    "    # tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(300)),#, return_sequences=False, dropout=0.25, recurrent_dropout=0.25)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "baseline.compile(loss=\"binary_crossentropy\", optimizer= tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=[\"accuracy\", tf.keras.metrics.FalsePositives(), tf.keras.metrics.FalseNegatives()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prediction_for_each_epoch(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        prediction = model.predict(np.expand_dims(testing_padded[idx], axis=0))\n",
    "        print(f\"\\n Prediction: {prediction[0][0]} Actual: {testing_labels[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence is 'finnish food worker union sel plan <OOV> collect <OOV> two day strike would begin 7 april 2010 finland .'\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 278ms/step loss: 0.6570 - accuracy: 0.6368 - false_positives: 25.0000 - false_negatives: 1858.00\n",
      "\n",
      " Prediction: 0.49398550391197205 Actual: 0\n",
      "164/164 [==============================] - 12s 37ms/step - loss: 0.6563 - accuracy: 0.6377 - false_positives: 25.0000 - false_negatives: 1870.0000 - val_loss: 0.6008 - val_accuracy: 0.6649 - val_false_positives: 39.0000 - val_false_negatives: 156.0000\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 23ms/step- loss: 0.5531 - accuracy: 0.7288 - false_positives: 420.0000 - false_negatives: 986.00\n",
      "\n",
      " Prediction: 0.49398550391197205 Actual: 0\n",
      "164/164 [==============================] - 4s 26ms/step - loss: 0.5537 - accuracy: 0.7282 - false_positives: 424.0000 - false_negatives: 998.0000 - val_loss: 0.5330 - val_accuracy: 0.7388 - val_false_positives: 53.0000 - val_false_negatives: 99.0000\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 18ms/step- loss: 0.4396 - accuracy: 0.8090 - false_positives: 391.0000 - false_negatives: 605.00\n",
      "\n",
      " Prediction: 0.49398550391197205 Actual: 0\n",
      "164/164 [==============================] - 4s 26ms/step - loss: 0.4401 - accuracy: 0.8086 - false_positives: 394.0000 - false_negatives: 607.0000 - val_loss: 0.5355 - val_accuracy: 0.7680 - val_false_positives: 67.0000 - val_false_negatives: 68.0000\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 18ms/step- loss: 0.3563 - accuracy: 0.8549 - false_positives: 349.0000 - false_negatives: 408.00\n",
      "\n",
      " Prediction: 0.49398550391197205 Actual: 0\n",
      "164/164 [==============================] - 4s 25ms/step - loss: 0.3561 - accuracy: 0.8551 - false_positives: 350.0000 - false_negatives: 408.0000 - val_loss: 0.5378 - val_accuracy: 0.7423 - val_false_positives: 111.0000 - val_false_negatives: 39.0000\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 19ms/step- loss: 0.2835 - accuracy: 0.8873 - false_positives: 259.0000 - false_negatives: 329.00\n",
      "\n",
      " Prediction: 0.49398550391197205 Actual: 0\n",
      "164/164 [==============================] - 4s 25ms/step - loss: 0.2832 - accuracy: 0.8874 - false_positives: 259.0000 - false_negatives: 330.0000 - val_loss: 0.5734 - val_accuracy: 0.7560 - val_false_positives: 84.0000 - val_false_negatives: 58.0000\n"
     ]
    }
   ],
   "source": [
    "stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "string = \"\"\n",
    "for word in testing_padded[idx]:\n",
    "    if word == 0:\n",
    "        string += \".\"\n",
    "        break\n",
    "    string += tokenizer.index_word[word] + \" \"\n",
    "print(f\"Sentence is '{string}'\")\n",
    "\n",
    "history = baseline.fit(training_padded, training_labels, batch_size=batch_size, epochs=epochs, validation_data=(testing_padded, testing_labels), callbacks=[stop, prediction_for_each_epoch()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding MaxPooling1D Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model_1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(300)),#, return_sequences=False, dropout=0.25, recurrent_dropout=0.25)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model_1.compile(loss=\"binary_crossentropy\", optimizer= tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=[\"accuracy\", tf.keras.metrics.FalsePositives(), tf.keras.metrics.FalseNegatives()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 291ms/step loss: 0.6570 - accuracy: 0.6368 - false_positives: 30.0000 - false_negatives: 1870.00\n",
      "\n",
      " Prediction: 0.49398550391197205 Actual: 0\n",
      "164/164 [==============================] - 12s 36ms/step - loss: 0.6570 - accuracy: 0.6368 - false_positives: 30.0000 - false_negatives: 1870.0000 - val_loss: 0.6242 - val_accuracy: 0.6615 - val_false_positives: 0.0000e+00 - val_false_negatives: 197.0000\n",
      "Epoch 2/50\n",
      "103/164 [=================>............] - ETA: 1s - loss: 0.5910 - accuracy: 0.6939 - false_positives: 263.0000 - false_negatives: 746.0000"
     ]
    }
   ],
   "source": [
    "history_1 = model_1.fit(training_padded, training_labels, batch_size=batch_size, epochs=epochs, validation_data=(testing_padded, testing_labels), callbacks=[stop, prediction_for_each_epoch()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()\n",
    "\n",
    "def plot_lr(history):\n",
    "    lrs = 1e-8 * (10 ** (np.arange(epochs) / 20))\n",
    "\n",
    "    # Set the figure size\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Set the grid\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot the loss in log scale\n",
    "    plt.semilogx(lrs, history.history[\"val_loss\"])\n",
    "\n",
    "    # Increase the tickmarks size\n",
    "    plt.tick_params('both', length=10, width=1, which='both')\n",
    "\n",
    "    # Set the plot boundaries\n",
    "    plt.axis([1e-8, 1e-3, 0, 2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_graphs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Jaysh\\OneDrive\\Desktop\\UberHackathon\\model\\training copy.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jaysh/OneDrive/Desktop/UberHackathon/model/training%20copy.ipynb#ch0000025?line=0'>1</a>\u001b[0m \u001b[39m#plot_lr(history)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Jaysh/OneDrive/Desktop/UberHackathon/model/training%20copy.ipynb#ch0000025?line=1'>2</a>\u001b[0m plot_graphs(history, \u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jaysh/OneDrive/Desktop/UberHackathon/model/training%20copy.ipynb#ch0000025?line=2'>3</a>\u001b[0m plot_graphs(history, \u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jaysh/OneDrive/Desktop/UberHackathon/model/training%20copy.ipynb#ch0000025?line=3'>4</a>\u001b[0m plot_graphs(history, \u001b[39m\"\u001b[39m\u001b[39mfalse_positives\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_graphs' is not defined"
     ]
    }
   ],
   "source": [
    "#plot_lr(history)\n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")\n",
    "plot_graphs(history, \"false_positives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentences):\n",
    "    if type(sentences) != list:\n",
    "        sentences = [sentences]\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = preProcess(sentences[i])\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "    \n",
    "    return model.predict(padded)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "[[0.07892309]]\n"
     ]
    }
   ],
   "source": [
    "print(predict(\"Wedbush Downgrades Redfin to Neutral, Lowers Price Target to $9\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open writeable files\n",
    "out_v = io.open('embeds/vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('embeds/meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "# Get the embedding layer from the model (i.e. first layer)\n",
    "embedding_layer = model.layers[0]\n",
    "\n",
    "# Get the weights of the embedding layer\n",
    "embedding_weights = embedding_layer.get_weights()[0]\n",
    "\n",
    "# Print the shape. Expected is (vocab_size, embedding_dim)\n",
    "print(embedding_weights.shape) \n",
    "\n",
    "# Initialize the loop. Start counting at `1` because `0` is just for the padding\n",
    "for word_num in range(1, len(tokenizer.word_index)):\n",
    "\n",
    "  # Get the word associated at the current index\n",
    "  word_name = tokenizer.index_word[word_num]\n",
    "\n",
    "  # Get the embedding weights associated with the current index\n",
    "  word_embedding = embedding_weights[word_num]\n",
    "\n",
    "  # Write the word name\n",
    "  out_m.write(word_name + \"\\n\")\n",
    "\n",
    "  # Write the word embedding\n",
    "  out_v.write('\\t'.join([str(x) for x in word_embedding]) + \"\\n\")\n",
    "\n",
    "# Close the files\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.save(\"models/model.h5\")\n",
    "with open(\"models/word_index.json\", \"w+\", encoding=\"utf-8\") as _file:\n",
    "    _file.write(json.dumps(tokenizer.word_index, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20de3a72cd82486f9818f29699300b403b2562cfa87c37eb33442eceae785fbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
